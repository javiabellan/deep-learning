{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - DataBlock Summary\n",
    "\n",
    "This notebook shows how to work `DataBlock.summary()` and what it can do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run once per session\n",
    "!pip install fastai2 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.vision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use `ImageWoof` like we did in previous notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = untar_data(URLs.IMAGEWOOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_dict = dict(\n",
    "  n02086240= 'Shih-Tzu',\n",
    "  n02087394= 'Rhodesian ridgeback',\n",
    "  n02088364= 'Beagle',\n",
    "  n02089973= 'English foxhound',\n",
    "  n02093754= 'Australian terrier',\n",
    "  n02096294= 'Border terrier',\n",
    "  n02099601= 'Golden retriever',\n",
    "  n02105641= 'Old English sheepdog',\n",
    "  n02111889= 'Samoyed',\n",
    "  n02115641= 'Dingo'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tfms = [*aug_transforms(size=224, max_warp=0), Normalize.from_stats(*imagenet_stats, cuda=False)]\n",
    "item_tfms = Resize(128)\n",
    "bs=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pets = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
    "                 get_items=get_image_files,\n",
    "                 splitter=RandomSplitter(),\n",
    "                 get_y=Pipeline([parent_label, lbl_dict.__getitem__]),\n",
    "                 item_tfms=item_tfms,\n",
    "                 batch_tfms=batch_tfms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to run `.summary`, we need to send in what our `DataBlock` expects. In this case it's a path (think how we make our `DataLoaders` from the `DataBlock`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting-up type transforms pipelines\n",
      "Collecting items from /root/.fastai/data/imagewoof2\n",
      "Found 12954 items\n",
      "2 datasets of sizes 10364,2590\n",
      "Setting up Pipeline: PILBase.create\n",
      "Setting up Pipeline: parent_label -> dict.__getitem__ -> Categorize\n",
      "\n",
      "Building one sample\n",
      "  Pipeline: PILBase.create\n",
      "    starting from\n",
      "      /root/.fastai/data/imagewoof2/train/n02096294/n02096294_3256.JPEG\n",
      "    applying PILBase.create gives\n",
      "      <fastai2.vision.core.PILImage image mode=RGB size=200x200 at 0x7F2A65CA2668>\n",
      "  Pipeline: parent_label -> dict.__getitem__ -> Categorize\n",
      "    starting from\n",
      "      /root/.fastai/data/imagewoof2/train/n02096294/n02096294_3256.JPEG\n",
      "    applying parent_label gives\n",
      "      n02096294\n",
      "    applying dict.__getitem__ gives\n",
      "      Border terrier\n",
      "    applying Categorize gives\n",
      "      TensorCategory(2)\n",
      "\n",
      "Final sample: (<fastai2.vision.core.PILImage image mode=RGB size=200x200 at 0x7F2A66EA4EF0>, TensorCategory(2))\n",
      "\n",
      "\n",
      "Setting up after_item: Pipeline: Resize -> ToTensor\n",
      "Setting up before_batch: Pipeline: \n",
      "Setting up after_batch: Pipeline: IntToFloatTensor -> AffineCoordTfm -> LightingTfm -> Normalize\n",
      "Could not do one pass in your dataloader, there is something wrong in it\n",
      "\n",
      "Building one batch\n",
      "Applying item_tfms to the first sample:\n",
      "  Pipeline: Resize -> ToTensor\n",
      "    starting from\n",
      "      (<fastai2.vision.core.PILImage image mode=RGB size=200x200 at 0x7F2A66EA45F8>, TensorCategory(2))\n",
      "    applying Resize gives\n",
      "      (<fastai2.vision.core.PILImage image mode=RGB size=128x128 at 0x7F2A66EA4DA0>, TensorCategory(2))\n",
      "    applying ToTensor gives\n",
      "      (TensorImage of size 3x128x128, TensorCategory(2))\n",
      "\n",
      "Adding the next 3 samples\n",
      "\n",
      "No before_batch transform to apply\n",
      "\n",
      "Collating items in a batch\n",
      "\n",
      "Applying batch_tfms to the batch built\n",
      "  Pipeline: IntToFloatTensor -> AffineCoordTfm -> LightingTfm -> Normalize\n",
      "    starting from\n",
      "      (TensorImage of size 4x3x128x128, TensorCategory([2, 7, 7, 2]))\n",
      "    applying IntToFloatTensor gives\n",
      "      (TensorImage of size 4x3x128x128, TensorCategory([2, 7, 7, 2]))\n",
      "    applying AffineCoordTfm gives\n",
      "      (TensorImage of size 4x3x224x224, TensorCategory([2, 7, 7, 2]))\n",
      "    applying LightingTfm gives\n",
      "      (TensorImage of size 4x3x224x224, TensorCategory([2, 7, 7, 2]))\n",
      "    applying Normalize gives\n",
      "      (TensorImage of size 4x3x224x224, TensorCategory([2, 7, 7, 2]))\n"
     ]
    }
   ],
   "source": [
    "pets.summary(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we find is it will go through **each** and every single part of our `DataBlock`, test it on an item, and we can see what popped out! **But!** What if we are using the `Datasets` instead? Let's go through how to utilize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [[PILImage.create], [parent_label, Categorize()]]\n",
    "item_tfms = [ToTensor(), Resize(128)]\n",
    "batch_tfms = [FlipItem(), RandomResizedCrop(128, min_scale=0.35),\n",
    "              IntToFloatTensor(), Normalize.from_stats(*imagenet_stats, cuda=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = get_image_files(path)\n",
    "split_idx = GrandparentSplitter(valid_name='val')(items)\n",
    "dsets = Datasets(items, tfms, splits=split_idx)\n",
    "dls = dsets.dataloaders(after_item=item_tfms, after_batch=batch_tfms, bs=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll want to grab the first item from our set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dsets.train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<fastai2.vision.core.PILImage image mode=RGB size=500x333 at 0x7F2A6592CE10>,\n",
       " TensorCategory(3))"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And pass it into any `after_item` or `after_batch` transform `Pipeline`. We can list them by calling them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline: Resize -> ToTensor"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls.train.after_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline: FlipItem -> RandomResizedCrop -> IntToFloatTensor -> Normalize"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls.train.after_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can pass in our item through the `Pipeline` like so:\n",
    "\n",
    "(`x[0]` has our input and `x[1]` has our `y`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resize TensorImage([[[[ 0.8961,  0.8961,  0.8789,  ...,  0.6563,  0.6221,  0.5536],\n",
      "          [ 0.7248,  0.6906,  0.6563,  ...,  0.6049,  0.6221,  0.6392],\n",
      "          [ 0.4851,  0.4679,  0.4166,  ...,  0.5707,  0.6049,  0.5707],\n",
      "          ...,\n",
      "          [ 0.1254, -0.3198, -0.5253,  ..., -1.7240, -1.8097, -1.7754],\n",
      "          [-0.5253, -1.1075, -1.1932,  ..., -1.4672, -1.7583, -1.6384],\n",
      "          [-0.8507, -1.2617, -0.9877,  ..., -1.5699, -1.6213, -1.7240]],\n",
      "\n",
      "         [[ 1.2381,  1.2381,  1.2206,  ...,  1.0455,  0.9755,  0.9230],\n",
      "          [ 1.0805,  1.0455,  1.0105,  ...,  0.9930,  1.0105,  1.0105],\n",
      "          [ 0.8179,  0.7829,  0.7129,  ...,  0.9230,  0.9755,  0.9405],\n",
      "          ...,\n",
      "          [ 0.5203, -0.1625, -0.5476,  ..., -1.1253, -1.1779, -1.3354],\n",
      "          [ 0.0651, -1.0553, -1.2654,  ..., -1.0728, -1.3179, -1.4055],\n",
      "          [-0.0924, -1.0028, -0.9503,  ..., -1.2304, -1.3880, -1.6155]],\n",
      "\n",
      "         [[ 1.5942,  1.5942,  1.5594,  ...,  1.4025,  1.3328,  1.2805],\n",
      "          [ 1.4548,  1.4025,  1.3502,  ...,  1.3502,  1.3677,  1.3677],\n",
      "          [ 1.1411,  1.1062,  1.0539,  ...,  1.2805,  1.3328,  1.2631],\n",
      "          ...,\n",
      "          [-0.0267, -0.7413, -1.0376,  ..., -1.5604, -1.6127, -1.6302],\n",
      "          [-0.5147, -1.2816, -1.4384,  ..., -1.4210, -1.6476, -1.6476],\n",
      "          [-0.8284, -1.3513, -1.2816,  ..., -1.2990, -1.6302, -1.6824]]]])\n",
      "ToTensor TensorImage([[[[ 0.8961,  0.8961,  0.8789,  ...,  0.6563,  0.6221,  0.5536],\n",
      "          [ 0.7248,  0.6906,  0.6563,  ...,  0.6049,  0.6221,  0.6392],\n",
      "          [ 0.4851,  0.4679,  0.4166,  ...,  0.5707,  0.6049,  0.5707],\n",
      "          ...,\n",
      "          [ 0.1254, -0.3198, -0.5253,  ..., -1.7240, -1.8097, -1.7754],\n",
      "          [-0.5253, -1.1075, -1.1932,  ..., -1.4672, -1.7583, -1.6384],\n",
      "          [-0.8507, -1.2617, -0.9877,  ..., -1.5699, -1.6213, -1.7240]],\n",
      "\n",
      "         [[ 1.2381,  1.2381,  1.2206,  ...,  1.0455,  0.9755,  0.9230],\n",
      "          [ 1.0805,  1.0455,  1.0105,  ...,  0.9930,  1.0105,  1.0105],\n",
      "          [ 0.8179,  0.7829,  0.7129,  ...,  0.9230,  0.9755,  0.9405],\n",
      "          ...,\n",
      "          [ 0.5203, -0.1625, -0.5476,  ..., -1.1253, -1.1779, -1.3354],\n",
      "          [ 0.0651, -1.0553, -1.2654,  ..., -1.0728, -1.3179, -1.4055],\n",
      "          [-0.0924, -1.0028, -0.9503,  ..., -1.2304, -1.3880, -1.6155]],\n",
      "\n",
      "         [[ 1.5942,  1.5942,  1.5594,  ...,  1.4025,  1.3328,  1.2805],\n",
      "          [ 1.4548,  1.4025,  1.3502,  ...,  1.3502,  1.3677,  1.3677],\n",
      "          [ 1.1411,  1.1062,  1.0539,  ...,  1.2805,  1.3328,  1.2631],\n",
      "          ...,\n",
      "          [-0.0267, -0.7413, -1.0376,  ..., -1.5604, -1.6127, -1.6302],\n",
      "          [-0.5147, -1.2816, -1.4384,  ..., -1.4210, -1.6476, -1.6476],\n",
      "          [-0.8284, -1.3513, -1.2816,  ..., -1.2990, -1.6302, -1.6824]]]])\n"
     ]
    }
   ],
   "source": [
    "for f in dls.train.after_item:\n",
    "  name = f.name\n",
    "  x = f(x)\n",
    "  print(name, x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlipItem TensorImage([[[176, 176, 175,  ..., 162, 160, 156],\n",
      "         [166, 164, 162,  ..., 159, 160, 161],\n",
      "         [152, 151, 148,  ..., 157, 159, 157],\n",
      "         ...,\n",
      "         [131, 105,  93,  ...,  23,  18,  20],\n",
      "         [ 93,  59,  54,  ...,  38,  21,  28],\n",
      "         [ 74,  50,  66,  ...,  32,  29,  23]],\n",
      "\n",
      "        [[187, 187, 186,  ..., 176, 172, 169],\n",
      "         [178, 176, 174,  ..., 173, 174, 174],\n",
      "         [163, 161, 157,  ..., 169, 172, 170],\n",
      "         ...,\n",
      "         [146, 107,  85,  ...,  52,  49,  40],\n",
      "         [120,  56,  44,  ...,  55,  41,  36],\n",
      "         [111,  59,  62,  ...,  46,  37,  24]],\n",
      "\n",
      "        [[195, 195, 193,  ..., 184, 180, 177],\n",
      "         [187, 184, 181,  ..., 181, 182, 182],\n",
      "         [169, 167, 164,  ..., 177, 180, 176],\n",
      "         ...,\n",
      "         [102,  61,  44,  ...,  14,  11,  10],\n",
      "         [ 74,  30,  21,  ...,  22,   9,   9],\n",
      "         [ 56,  26,  30,  ...,  29,  10,   7]]], dtype=torch.uint8)\n",
      "RandomResizedCrop TensorImage([[[176, 176, 175,  ..., 162, 160, 156],\n",
      "         [166, 164, 162,  ..., 159, 160, 161],\n",
      "         [152, 151, 148,  ..., 157, 159, 157],\n",
      "         ...,\n",
      "         [131, 105,  93,  ...,  23,  18,  20],\n",
      "         [ 93,  59,  54,  ...,  38,  21,  28],\n",
      "         [ 74,  50,  66,  ...,  32,  29,  23]],\n",
      "\n",
      "        [[187, 187, 186,  ..., 176, 172, 169],\n",
      "         [178, 176, 174,  ..., 173, 174, 174],\n",
      "         [163, 161, 157,  ..., 169, 172, 170],\n",
      "         ...,\n",
      "         [146, 107,  85,  ...,  52,  49,  40],\n",
      "         [120,  56,  44,  ...,  55,  41,  36],\n",
      "         [111,  59,  62,  ...,  46,  37,  24]],\n",
      "\n",
      "        [[195, 195, 193,  ..., 184, 180, 177],\n",
      "         [187, 184, 181,  ..., 181, 182, 182],\n",
      "         [169, 167, 164,  ..., 177, 180, 176],\n",
      "         ...,\n",
      "         [102,  61,  44,  ...,  14,  11,  10],\n",
      "         [ 74,  30,  21,  ...,  22,   9,   9],\n",
      "         [ 56,  26,  30,  ...,  29,  10,   7]]], dtype=torch.uint8)\n",
      "IntToFloatTensor TensorImage([[[0.6902, 0.6902, 0.6863,  ..., 0.6353, 0.6275, 0.6118],\n",
      "         [0.6510, 0.6431, 0.6353,  ..., 0.6235, 0.6275, 0.6314],\n",
      "         [0.5961, 0.5922, 0.5804,  ..., 0.6157, 0.6235, 0.6157],\n",
      "         ...,\n",
      "         [0.5137, 0.4118, 0.3647,  ..., 0.0902, 0.0706, 0.0784],\n",
      "         [0.3647, 0.2314, 0.2118,  ..., 0.1490, 0.0824, 0.1098],\n",
      "         [0.2902, 0.1961, 0.2588,  ..., 0.1255, 0.1137, 0.0902]],\n",
      "\n",
      "        [[0.7333, 0.7333, 0.7294,  ..., 0.6902, 0.6745, 0.6627],\n",
      "         [0.6980, 0.6902, 0.6824,  ..., 0.6784, 0.6824, 0.6824],\n",
      "         [0.6392, 0.6314, 0.6157,  ..., 0.6627, 0.6745, 0.6667],\n",
      "         ...,\n",
      "         [0.5725, 0.4196, 0.3333,  ..., 0.2039, 0.1922, 0.1569],\n",
      "         [0.4706, 0.2196, 0.1725,  ..., 0.2157, 0.1608, 0.1412],\n",
      "         [0.4353, 0.2314, 0.2431,  ..., 0.1804, 0.1451, 0.0941]],\n",
      "\n",
      "        [[0.7647, 0.7647, 0.7569,  ..., 0.7216, 0.7059, 0.6941],\n",
      "         [0.7333, 0.7216, 0.7098,  ..., 0.7098, 0.7137, 0.7137],\n",
      "         [0.6627, 0.6549, 0.6431,  ..., 0.6941, 0.7059, 0.6902],\n",
      "         ...,\n",
      "         [0.4000, 0.2392, 0.1725,  ..., 0.0549, 0.0431, 0.0392],\n",
      "         [0.2902, 0.1176, 0.0824,  ..., 0.0863, 0.0353, 0.0353],\n",
      "         [0.2196, 0.1020, 0.1176,  ..., 0.1137, 0.0392, 0.0275]]])\n",
      "Normalize TensorImage([[[[ 0.8961,  0.8961,  0.8789,  ...,  0.6563,  0.6221,  0.5536],\n",
      "          [ 0.7248,  0.6906,  0.6563,  ...,  0.6049,  0.6221,  0.6392],\n",
      "          [ 0.4851,  0.4679,  0.4166,  ...,  0.5707,  0.6049,  0.5707],\n",
      "          ...,\n",
      "          [ 0.1254, -0.3198, -0.5253,  ..., -1.7240, -1.8097, -1.7754],\n",
      "          [-0.5253, -1.1075, -1.1932,  ..., -1.4672, -1.7583, -1.6384],\n",
      "          [-0.8507, -1.2617, -0.9877,  ..., -1.5699, -1.6213, -1.7240]],\n",
      "\n",
      "         [[ 1.2381,  1.2381,  1.2206,  ...,  1.0455,  0.9755,  0.9230],\n",
      "          [ 1.0805,  1.0455,  1.0105,  ...,  0.9930,  1.0105,  1.0105],\n",
      "          [ 0.8179,  0.7829,  0.7129,  ...,  0.9230,  0.9755,  0.9405],\n",
      "          ...,\n",
      "          [ 0.5203, -0.1625, -0.5476,  ..., -1.1253, -1.1779, -1.3354],\n",
      "          [ 0.0651, -1.0553, -1.2654,  ..., -1.0728, -1.3179, -1.4055],\n",
      "          [-0.0924, -1.0028, -0.9503,  ..., -1.2304, -1.3880, -1.6155]],\n",
      "\n",
      "         [[ 1.5942,  1.5942,  1.5594,  ...,  1.4025,  1.3328,  1.2805],\n",
      "          [ 1.4548,  1.4025,  1.3502,  ...,  1.3502,  1.3677,  1.3677],\n",
      "          [ 1.1411,  1.1062,  1.0539,  ...,  1.2805,  1.3328,  1.2631],\n",
      "          ...,\n",
      "          [-0.0267, -0.7413, -1.0376,  ..., -1.5604, -1.6127, -1.6302],\n",
      "          [-0.5147, -1.2816, -1.4384,  ..., -1.4210, -1.6476, -1.6476],\n",
      "          [-0.8284, -1.3513, -1.2816,  ..., -1.2990, -1.6302, -1.6824]]]])\n"
     ]
    }
   ],
   "source": [
    "for f in dls.train.after_batch:\n",
    "  name = f.name\n",
    "  x = f(x)\n",
    "  print(name, x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
