<h1 align="center">History of Deep Learning</h1>

## `1943` First neuron
<img width="250" align="right" src="https://machinelearningknowledge.ai/wp-content/uploads/2019/11/McCulloch-Pitts-1.jpg">

[Walter Pitts](https://en.wikipedia.org/wiki/Walter_Pitts) and [Warren McCulloch](https://en.wikipedia.org/wiki/Warren_Sturgis_McCulloch) in their paper, *“A Logical Calculus of the Ideas Immanent in Nervous Activity”* shows the mathematical model of biological neuron. This McCulloch Pitts Neuron has very limited capability and has no learning mechanism. Yet it will lay the foundation for artificial neural network & deep learning.

## `1957` Perceptron
<img width="250" align="right" src="https://machinelearningknowledge.ai/wp-content/uploads/2019/11/Frank-Rosenblatt-Perceptron.jpg">

[Frank Rosenblatt](https://en.wikipedia.org/wiki/Frank_Rosenblatt), in his paper “The Perceptron: A Perceiving and Recognizing Automaton”, shows the new avatar of McCulloch-Pitts neuron – ‘Perceptron’ that had true learning capabilities to do binary classification on it’s own. This inspires the revolution in research of shallow neural network for years to come, till first AI winter.

## `1960` First Backpropagation Model
Henry J. Kelley in his paper, “Gradient Theory of Optimal Flight Paths” shows the first ever version of continuous backpropagation model. His model is in context to Control Theory, yet it lays the foundation for further refinement in the model and would be used in ANN in future years.

## `1962` Backpropagation With Chain Rule
Backpropagation with Chain Rule
Stuart Dreyfus in his paper, “The numerical solution of variational problems” shows a backpropagation model that uses simple derivative chain rule, instead of dynamic programming which earlier backpropagation models were using. This is yet another small step that strengthens the future of deep learning.

## `1965` Birth of Multilayer Neural Network
Alexey Grigoryevich Ivakhnenko along with Valentin Grigorʹevich Lapa, creates hierarchical representation of neural network that uses polynomial activation function and are trained using Group Method of Data Handling (GMDH). It is now considered as the first ever multi-layer perceptron and Ivakhnenko is often considered as father of deep learning.

## `1969` The Fall of Perceptron
Marvin Minsky and Seymour Papert publishes the book “Perceptrons” in which they show that Rosenblatt’s perceptron cannot solve complicated functions like XOR. For such function perceptrons should be placed in multiple hidden layers which compromises perceptron learning algorithm. This setback triggers the winter of neural network research.


### References
- [Brief History of Deep Learning from 1943-2019 (Timeline)](https://machinelearningknowledge.ai/brief-history-of-deep-learning/?fbclid=IwAR09wRExjQla-ZKutxbVIDptD6vfQbaAclXIWfh8X5SjyWW61Zu76eJB2A0)
- Jürgen Schmidhuber
  - http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html
  - http://people.idsia.ch/~juergen/2010s-our-decade-of-deep-learning.html
  - http://people.idsia.ch/~juergen/critique-honda-prize-hinton.html
