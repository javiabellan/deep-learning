# Diseño de la arquitectura

Otra aspecto a tener en cuenta es la estructura de la red neuronal: ¿Cuántas unidades usar, y cómo conectarlas?
Las unidades se agrupan por capas y estas se conectan una tras otra.

<p align="center"><img src="https://icdn5.digitaltrends.com/image/artificial_neural_network_1-791x388.jpg" width="50%"/></p>

Realmente elegimos la estructura de las capas ocultas: ¿Cúantas capas usar? y ¿cuántas unidades en cada capa?
Porque los tamaños de las capas de entrada y de salida es fijo, ya que viene determinado por la función que queremos aproximar,
(qué datos tomamos como entrada, y qué datos producimos como salida. Matemáticamente se dice que se pasa de un espacio finito n-dimensional o otro). Recuerda que la capa de entrada la forman sólo datos, no son perceptrones.

> ### Teorema universal de la aproximación
> **Una sóla capa oculta es suficiente para aproximar cualquer función**, (con la precisión deseada).

Aunque teóricamente una sóla capa oculta sea suficiente, ésta necesitaría muchísimas unidades.
En la práctica se usan varias capas porque usan muchos menos parámetros y generalizan mejor.
Pero demasiadas capas implica que la red es más dificil de optimizar.

El teorma se cumple para caulquier función de acctivación (incluidas las ReLUs)

Además aunque una una red neuronal grande sea capaz de aprender cualqueier función,
no implica que su algoritmo de aprendizaje sea capaz de hacerlo. Éste puede fallar por 2 razones:
1. No sea capaz de encontrar los valores de los parámetros que se corresponden con la función a aproximar.
2. El algoritmo elija otra función a consecunecia del overfitting

> ### Nota
> También existe la posibilidad de no usar capa oculta, pero entonces solo podríamos aproximar funciones lineales
> (rectas, planos o hiperplanos). Desfortunadamente la mayoría de las funciones que queremos aproximar son **no-lineales**. 

#### Elegir el número de capas
Normalamente, se eligen las redes neuronales en machine learning porque se quiere conseguir
una función que es una composición de varias funciones que se desconocen, esto implica varias capas.
Se puede interpretar como que la red va aprendiendo en cada función (capa) las caracteristicas que le interesan
para proporcinarsalas a la siguiente función (capa) para que pueda extraer nueva información.
Por lo tanto cuanto más distantes sean los datos de entrada con los datos de salida,
(que implican extraer muchos pasos y caracteristicas intermedias), da una idea de la cantidad de capas a usar.

Pero no tenemeos tanta intuición para seber el numero adecuado de capas, así que
la arqutectura se suele obtener por prueba y error:

Añadir más capas no emperora la precisión pero implica mas parametros, mas posibilidad de overffittng y tambén mas dificil de optimizar

#### Elegir el tamaño de cada capa
El tamaño de cada capa alberga la cantidad de caracteristicas a extraer en ese paso/capa/función.
Tampoco está claro, se suele elgir algo intermedio entre el tamaño de la capa de entrada y la capa de salida, quizas un poco más, nose.

