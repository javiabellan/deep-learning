# Funciones de activación

Vamos a ver como elegir las unidades ocultas de las capas ocultas.

Elegir el tipo de función de activación es un tema muy estudiado en la invesitgación.
Hay muchos tipos, prueba y error.

> ### Funciones de activación no diferenciables
>
> Muchas de ellas no son diferneciables, por ejemplo `g(z) = max(0,z)` no es diferenciable en 0.
> Esto parecería ser un problema para el descenso por gradiente, pero en la práctica, el algoritmo funcion suficentemente bien:
>
> Because we do not expect training to actually reach a point where the gradient is 0,
> it is acceptable for the minima of the cost function to correspond to points with undeﬁned gradient.
>
> Hidden units that are not diﬀerentiable are usually nondiﬀerentiable at only a small number of points.
>
> Algo de que la derivada a ambos lasdo debe ser igual o perceida, pero aqui (cuando el punto es 0) a la izq es 0 y la derecha es 1,
> pero el sofware obvia esto y devielve la derivada de un lado solo en lugar de decir que hay un error

## ReLU

Las Rectiﬁed linear units o ReLU son la función de accivación `g(z) = max(0,z)`.
Son una excelente elección, ya que son fácil de optimizar.

![](https://nn.readthedocs.io/en/rtd/image/relu.png)

Es similar a la función e identidad, salvo que devuleve 0 cuando la entrada es negativa.




## Referencias

* http://www.deeplearningbook.org/contents/mlp.html
* https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6
