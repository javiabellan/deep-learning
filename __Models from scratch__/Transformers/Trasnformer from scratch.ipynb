{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">1. Pretraining a LM (few days)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_embeddings, num_max_positions, num_heads, num_layers, dropout, causal):\n",
    "        \n",
    "        super().__init__()\n",
    "                \n",
    "        #### Initial input embedding\n",
    "        self.tokens_embeddings   = nn.Embedding(num_embeddings,   embed_dim)\n",
    "        self.position_embeddings = nn.Embedding(num_max_positions, embed_dim)\n",
    "        \n",
    "        #### Mask\n",
    "        self.causal = causal\n",
    "            \n",
    "        #### Transformer block components\n",
    "        self.attentions    = nn.ModuleList()\n",
    "        self.feed_forwards = nn.ModuleList()\n",
    "        self.layer_norms_1 = nn.ModuleList()\n",
    "        self.layer_norms_2 = nn.ModuleList()\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        \n",
    "        for _ in range(num_layers):\n",
    "            self.attentions.append(nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout))\n",
    "            \n",
    "            self.feed_forwards.append(nn.Sequential(nn.Linear(embed_dim, hidden_dim),\n",
    "                                                    nn.ReLU(),\n",
    "                                                    nn.Linear(hidden_dim, embed_dim)))\n",
    "            \n",
    "            self.layer_norms_1.append(nn.LayerNorm(embed_dim, eps=1e-12))\n",
    "            self.layer_norms_2.append(nn.LayerNorm(embed_dim, eps=1e-12))\n",
    "\n",
    "            \n",
    "            \n",
    "    # x            has shape [seq length, batch]\n",
    "    # padding_mask has shape [batch, seq length]\n",
    "    def forward(self, x, padding_mask=None):\n",
    "        \n",
    "        #### Initial input embedding\n",
    "        positions = torch.arange(len(x), device=x.device).unsqueeze(-1)\n",
    "        h = self.tokens_embeddings(x)\n",
    "        h = h + self.position_embeddings(positions).expand_as(h)\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        #### Mask\n",
    "        attn_mask = None\n",
    "        if self.causal:\n",
    "            attn_mask = torch.full((len(x), len(x)), -float('Inf'), device=h.device, dtype=h.dtype)\n",
    "            attn_mask = torch.triu(attn_mask, diagonal=1)\n",
    "\n",
    "        #### Transformer block components\n",
    "        for layer_norm_1, attention, layer_norm_2, feed_forward in zip(self.layer_norms_1, self.attentions,\n",
    "                                                                       self.layer_norms_2, self.feed_forwards):\n",
    "            h = layer_norm_1(h)\n",
    "            x, _ = attention(h, h, h, attn_mask=attn_mask, need_weights=False, key_padding_mask=padding_mask)\n",
    "            x = self.dropout(x)\n",
    "            h = x + h\n",
    "\n",
    "            h = layer_norm_2(h)\n",
    "            x = feed_forward(h)\n",
    "            x = self.dropout(x)\n",
    "            h = x + h\n",
    "            print(h)\n",
    "            \n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer with LM head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerWithLMHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = Transformer(config.embed_dim, config.hidden_dim, config.num_embeddings,\n",
    "                                       config.num_max_positions, config.num_heads, config.num_layers,\n",
    "                                       config.dropout, causal=not config.mlm)\n",
    "\n",
    "        self.lm_head = nn.Linear(config.embed_dim, config.num_embeddings, bias=False)\n",
    "        self.apply(self.init_weights)\n",
    "        self.tie_weights()\n",
    "\n",
    "    def tie_weights(self):\n",
    "        self.lm_head.weight = self.transformer.tokens_embeddings.weight\n",
    "\n",
    "    def init_weights(self, module):\n",
    "        \"\"\" initialize weights - nn.MultiheadAttention is already initalized by PyTorch (xavier) \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding, nn.LayerNorm)):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "        if isinstance(module, (nn.Linear, nn.LayerNorm)) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, labels=None, padding_mask=None):\n",
    "        \"\"\" x has shape [seq length, batch], padding_mask has shape [batch, seq length] \"\"\"\n",
    "        hidden_states = self.transformer(x, padding_mask)\n",
    "        logits = self.lm_head(hidden_states)\n",
    "\n",
    "        if labels is not None:\n",
    "            shift_logits = logits[:-1] if self.transformer.causal else logits\n",
    "            shift_labels = labels[1:] if self.transformer.causal else labels\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            return logits, loss\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = pytorch_transformers.BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"embed_dim\":         410,\n",
    "    \"hidden_dim\":        2100,\n",
    "    \"num_max_positions\": 256,\n",
    "    \"num_embeddings\":    len(tokenizer.vocab), # 30522 tokens\n",
    "    \"num_heads\":         10,\n",
    "    \"num_layers\":        16,\n",
    "    \"dropout\":           0.1,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"batch_size\":        16,\n",
    "    \"lr\":                2.5e-4,\n",
    "    \"max_norm\":          1.0,\n",
    "    \"n_epochs\":          50,\n",
    "    \"n_warmup\":          1000,\n",
    "    \"mlm\":               False,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"device\":            \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"log_dir\":           \"./\",\n",
    "    \"dataset_cache\":     \"./dataset_cache.bin\"\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
